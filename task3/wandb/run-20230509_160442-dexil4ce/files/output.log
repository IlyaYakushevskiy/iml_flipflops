
Epoch 1: Training Loss: 0.6934, Validation Loss: 0.6931
Epoch 2: Training Loss: 0.6847, Validation Loss: 0.6782
Epoch 3: Training Loss: 0.6548, Validation Loss: 0.6657
Epoch 4: Training Loss: 0.6695, Validation Loss: 0.6650
Epoch 5: Training Loss: 0.6323, Validation Loss: 0.6596
Epoch 6: Training Loss: 0.6393, Validation Loss: 0.6580
Epoch 7: Training Loss: 0.6545, Validation Loss: 0.6562
Epoch 8: Training Loss: 0.6614, Validation Loss: 0.6552
Epoch 9: Training Loss: 0.6342, Validation Loss: 0.6527
Epoch 10: Training Loss: 0.6624, Validation Loss: 0.6527
Epoch 11: Training Loss: 0.6551, Validation Loss: 0.6420
Epoch 12: Training Loss: 0.6592, Validation Loss: 0.6330
Epoch 13: Training Loss: 0.6072, Validation Loss: 0.6214
Epoch 14: Training Loss: 0.6614, Validation Loss: 0.6171
Epoch 15: Training Loss: 0.6031, Validation Loss: 0.6120
Epoch 16: Training Loss: 0.6325, Validation Loss: 0.6106
Epoch 17: Training Loss: 0.5450, Validation Loss: 0.6086
Epoch 18: Training Loss: 0.5488, Validation Loss: 0.6031
Epoch 19: Training Loss: 0.5982, Validation Loss: 0.6046
Epoch 20: Training Loss: 0.5586, Validation Loss: 0.6021
Epoch 21: Training Loss: 0.5854, Validation Loss: 0.6026
Epoch 22: Training Loss: 0.6206, Validation Loss: 0.5971
Epoch 23: Training Loss: 0.6304, Validation Loss: 0.5945
Epoch 24: Training Loss: 0.6006, Validation Loss: 0.5940
Epoch 25: Training Loss: 0.6116, Validation Loss: 0.5942
Epoch 26: Training Loss: 0.5854, Validation Loss: 0.5908
Epoch 27: Training Loss: 0.5849, Validation Loss: 0.5898
Epoch 28: Training Loss: 0.6233, Validation Loss: 0.5855
Epoch 29: Training Loss: 0.5591, Validation Loss: 0.5900
Epoch 30: Training Loss: 0.5993, Validation Loss: 0.5889
Epoch 31: Training Loss: 0.6009, Validation Loss: 0.5910
Epoch 32: Training Loss: 0.5758, Validation Loss: 0.5872
Epoch 33: Training Loss: 0.5986, Validation Loss: 0.5886
Epoch 34: Training Loss: 0.6330, Validation Loss: 0.5883
Epoch 35: Training Loss: 0.6643, Validation Loss: 0.5824
Epoch 36: Training Loss: 0.5205, Validation Loss: 0.5831
Epoch 37: Training Loss: 0.5774, Validation Loss: 0.5828
Epoch 38: Training Loss: 0.6227, Validation Loss: 0.5790
Epoch 39: Training Loss: 0.6685, Validation Loss: 0.5846
Epoch 40: Training Loss: 0.6063, Validation Loss: 0.5801
Epoch 41: Training Loss: 0.5675, Validation Loss: 0.5805
Epoch 42: Training Loss: 0.6026, Validation Loss: 0.5787
Epoch 43: Training Loss: 0.5738, Validation Loss: 0.5834
Epoch 44: Training Loss: 0.5719, Validation Loss: 0.5798
Epoch 45: Training Loss: 0.5909, Validation Loss: 0.5804
Epoch 46: Training Loss: 0.5441, Validation Loss: 0.5777
Epoch 47: Training Loss: 0.6286, Validation Loss: 0.5756
Epoch 48: Training Loss: 0.6116, Validation Loss: 0.5799
Epoch 49: Training Loss: 0.5668, Validation Loss: 0.5768
Epoch 50: Training Loss: 0.5618, Validation Loss: 0.5779
Epoch 51: Training Loss: 0.5691, Validation Loss: 0.5749
Epoch 52: Training Loss: 0.6208, Validation Loss: 0.5710
Epoch 53: Training Loss: 0.5806, Validation Loss: 0.5782
Epoch 54: Training Loss: 0.5664, Validation Loss: 0.5734
Epoch 55: Training Loss: 0.5591, Validation Loss: 0.5719
Epoch 56: Training Loss: 0.5704, Validation Loss: 0.5716
Epoch 57: Training Loss: 0.5902, Validation Loss: 0.5731
Epoch 58: Training Loss: 0.5844, Validation Loss: 0.5762
Epoch 59: Training Loss: 0.6158, Validation Loss: 0.5751
Epoch 60: Training Loss: 0.5962, Validation Loss: 0.5736
Epoch 61: Training Loss: 0.6148, Validation Loss: 0.5718
Epoch 62: Training Loss: 0.5957, Validation Loss: 0.5764
Epoch 63: Training Loss: 0.6686, Validation Loss: 0.5718
Epoch 64: Training Loss: 0.5823, Validation Loss: 0.5762
Epoch 65: Training Loss: 0.5410, Validation Loss: 0.5699
Epoch 66: Training Loss: 0.5707, Validation Loss: 0.5725
Epoch 67: Training Loss: 0.6416, Validation Loss: 0.5734
Epoch 68: Training Loss: 0.6240, Validation Loss: 0.5723
Epoch 69: Training Loss: 0.5930, Validation Loss: 0.5690
Epoch 70: Training Loss: 0.5883, Validation Loss: 0.5712
Epoch 71: Training Loss: 0.6111, Validation Loss: 0.5686
Epoch 72: Training Loss: 0.6295, Validation Loss: 0.5688
Epoch 73: Training Loss: 0.6362, Validation Loss: 0.5702
Epoch 74: Training Loss: 0.5632, Validation Loss: 0.5700
Epoch 75: Training Loss: 0.6292, Validation Loss: 0.5666
Epoch 76: Training Loss: 0.4955, Validation Loss: 0.5653
Epoch 77: Training Loss: 0.6178, Validation Loss: 0.5685
Epoch 78: Training Loss: 0.5999, Validation Loss: 0.5676
Epoch 79: Training Loss: 0.6250, Validation Loss: 0.5657
Epoch 80: Training Loss: 0.5967, Validation Loss: 0.5691
Epoch 81: Training Loss: 0.5401, Validation Loss: 0.5701
Epoch 82: Training Loss: 0.6688, Validation Loss: 0.5627
Epoch 83: Training Loss: 0.5979, Validation Loss: 0.5676
Epoch 84: Training Loss: 0.5617, Validation Loss: 0.5650
Epoch 85: Training Loss: 0.6495, Validation Loss: 0.5646
Epoch 86: Training Loss: 0.5228, Validation Loss: 0.5618
Epoch 87: Training Loss: 0.6479, Validation Loss: 0.5645
Epoch 88: Training Loss: 0.5555, Validation Loss: 0.5668
Epoch 89: Training Loss: 0.6208, Validation Loss: 0.5653
Epoch 90: Training Loss: 0.6562, Validation Loss: 0.5642
Epoch 91: Training Loss: 0.6530, Validation Loss: 0.5638
Epoch 92: Training Loss: 0.6519, Validation Loss: 0.5622
Epoch 93: Training Loss: 0.5815, Validation Loss: 0.5624
Epoch 94: Training Loss: 0.5965, Validation Loss: 0.5613
Epoch 95: Training Loss: 0.5970, Validation Loss: 0.5622
Epoch 96: Training Loss: 0.6331, Validation Loss: 0.5620
Epoch 97: Training Loss: 0.6398, Validation Loss: 0.5612
Epoch 98: Training Loss: 0.5641, Validation Loss: 0.5609
Epoch 99: Training Loss: 0.6308, Validation Loss: 0.5594
Traceback (most recent call last):
  File "/Users/ilya/Desktop/iml_flipflops/task3/sol_Lauriane.py", line 277, in <module>
    model = train_model(train_loader,n_epochs )
  File "/Users/ilya/Desktop/iml_flipflops/task3/sol_Lauriane.py", line 193, in train_model
    optimizer.step()
  File "/opt/homebrew/Caskroom/miniforge/base/envs/ml_cv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/ml_cv/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/ml_cv/lib/python3.10/site-packages/torch/optim/adam.py", line 141, in step
    adam(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/ml_cv/lib/python3.10/site-packages/torch/optim/adam.py", line 281, in adam
    func(params,
  File "/opt/homebrew/Caskroom/miniforge/base/envs/ml_cv/lib/python3.10/site-packages/torch/optim/adam.py", line 391, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt